{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5209ea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b93bdadc-f231-443a-a107-581b19e788b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch==2.4.1+cu121 torchvision==0.19.1+cu121 torchaudio==2.4.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4261cee2-5c84-42d4-aa7f-762412c65bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  -U -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets qwen-vl-utils wandb accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4f84c-6a1b-4de6-b804-a0fcac8ef979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtungpthe187331\u001b[0m (\u001b[33mtungpthe187331-fpt-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "467726ed-6ee1-493b-a1a9-bc22ff0d157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from trl import SFTConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baf52b43-e396-4f5d-850c-123ccf95bffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c822097af77d4429938afadd3ffe2621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/500 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d55f915d5c466b9dfdc43b471297fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/1.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec88f6a024e14e728eec9bd938b724db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval-00000-of-00001.parquet:   0%|          | 0.00/10.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c78dfc4334da4fe280c8013e417c3143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2642409f11ec4e5f90ddaf3fd3577a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating eval split:   0%|          | 0/45 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset, eval_dataset = load_dataset(\"phmtung/Qwen2-VL-OCR-ViDocs\", split=[\"train\", \"eval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60f8cb7-f39c-4aa4-a74a-10ee4cd407ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample):\n",
    "    images = sample[\"image\"] if isinstance(sample[\"image\"], list) else [sample[\"image\"]]\n",
    "    user_content = [{\"type\": \"image\", \"image\": img} for img in images]\n",
    "    user_content.append({\"type\": \"text\", \"text\": sample[\"query\"]})\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_content,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"]}],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042fe329-fec6-4e3b-9095-22f96f5206ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [format_data(sample) for sample in train_dataset]\n",
    "eval_dataset = [format_data(sample) for sample in eval_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f43f7a2c-19ae-4679-acc2-9581ba889039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': [{'type': 'image',\n",
       "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2829x1910>},\n",
       "   {'type': 'text',\n",
       "    'text': 'You are a highly precise assistant specializing in Vietnamese OCR. Your task is to **accurately extract text** from vehicle inspection report image. Please follow these strict requirements:  \\n\\n- Extract text **exactly as it appears** in the image, including capitalization, special characters, and formatting.  \\n- **Do not modify, infer, or guess** any missing values. If a field is unreadable or missing, return an empty string (\"\").  \\n- Ensure all extracted text is **error-free** and corresponds correctly to the predefined fields.\\n- Maintain the exact JSON structure and **output only valid JSON**.\\n- **Note:** The image may be rotated, distorted, damaged, low-resolution, or affected by lighting inconsistencies (e.g., glare, shadows, overexposure).\\n\\n### **JSON Output Format:**\\n{\\n    \"PHƯƠNG TIỆN (VEHICLE)\": {\\n        \"Biển đăng kí (Registration Number)\": \"<str Vehicle registration number>\",\\n        \"Số quản lý (Vehicle Inspection No.)\": \"<str Unique inspection number>\",\\n        \"Loại phương tiện (Type)\": \"<str Vehicle type>\",\\n        \"Nhãn hiệu (Mark)\": \"<str Vehicle brand>\",\\n        \"Số loại (Model code)\": \"<str Model code>\",\\n        \"Số máy (Engine Number)\": \"<str Engine serial number>\",\\n        \"Số khung (Chassis Number)\": \"<str Chassis serial number>\",\\n        \"Năm, Nước sản xuất (Manufactured Year and Country)\": \"<str Year and country of manufacture>\",\\n        \"Niên hạn SD (Lifetime limit to)\": \"<str Vehicle usage limit>\",\\n        \"Kinh doanh vận tải (Commercial Use)\": \"<bool \\'Có\\' if marked \\'x\\', \\'Không\\' if \\'-\\' >\",\\n        \"Cải tạo (Modification)\": \"<bool \\'Có\\' if marked \\'x\\', \\'Không\\' if \\'-\\' >\"\\n    },\\n    \"THÔNG SỐ KỸ THUẬT (SPECIFICATIONS)\": {\\n        \"Công thức bánh xe (Wheel Formula)\": \"<str Wheel configuration>\",\\n        \"Vết bánh xe (Wheel Tread)\": \"<str Wheel track width (mm)>\",\\n        \"Kích thước bao (Overall Dimension)\": \"<str Vehicle dimensions (L×W×H in mm)>\",\\n        \"Kích thước lòng thùng xe (Inside cargo container dimension)\": \"<str Cargo space dimensions (L×W×H in mm)>\",\\n        \"Chiều dài cơ sở (Wheelbase)\": \"<str Distance between axles (mm)>\",\\n        \"Khối lượng bản thân (Kerb mass)\": \"<str Empty vehicle weight (kg)>\",\\n        \"Khối lượng hàng CC theo TK/CP TGGT (Design/Authorized payload)\": \"<str Maximum allowed payload (kg)>\",\\n        \"Khối lượng toàn bộ theo TK/CP TGGT (Design/Authorized total mass)\": \"<str Maximum allowed total weight (kg)>\",\\n        \"Khối lượng kéo theo TK/CP TGGT (Design/Authorized towed mass)\": \"<str Maximum allowed towed mass (kg)>\",\\n        \"Số người cho phép chở (Permissible No. of Pers Carried: seat, stood place, laying place)\": \"<str Number of passengers allowed>\",\\n        \"Loại nhiên liệu (Type of Fuel Used)\": \"<str Type of fuel>\",\\n        \"Thể tích làm việc của động cơ (Engine Displacement)\": \"<str Engine capacity (cm3)>\",\\n        \"Công suất lớn nhất/tốc độ quay (Max. output/rpm)\": \"<str Maximum power and rpm (Max. output(kW)/rpm(vpl))>\",\\n        \"Số sê-ri (No.)\": \"<str Serial number>\",\\n        \"Số lượng lốp, cỡ lốp/trục (Number of tires; tire size/axle)\": \"<str Tire quantity and size>\",\\n        \"Có lắp thiết bị giám sát hành trình (Equipped with Tachograph)\": \"<bool \\'Có\\' if marked \\'x\\', \\'Không\\' if \\'-\\' >\",\\n        \"Không cấp tem kiểm định (Inspection stamp was not issued)\": \"<bool \\'Có\\' if marked \\'x\\', \\'Không\\' if \\'-\\' >\",\\n        \"Ghi chú\": \"<str Additional notes>\"\\n    },\\n    \"THÔNG TIN KHÁC\": {\\n        \"Số phiếu kiểm định (Inspection Report No)\": \"<str Unique report number>\",\\n        \"Có hiệu lực đến ngày (Valid until)\": \"<str Expiry date (DD/MM/YYYY)>\",\\n        \"Issued on Day/Month/Year\": \"<str Issuance date (DD/MM/YYYY)>\"\\n    }\\n}\\n'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': '{\\n    \"PHƯƠNG TIỆN (VEHICLE)\": {\\n        \"Biển đăng kí (Registration Number)\": \"30G-670.69\",\\n        \"Số quản lý (Vehicle Inspection No.)\": \"2901S-043023\",\\n        \"Loại phương tiện (Type)\": \"ô tô con\",\\n        \"Nhãn hiệu (Mark)\": \"VINFAST\",\\n        \"Số loại (Model code)\": \"LUX A2.0 / S5B1ALRVN\",\\n        \"Số máy (Engine Number)\": \"200160046VF20SED\",\\n        \"Số khung (Chasis Number)\": \"RPXAB1RSFKV008932\",\\n        \"Năm, Nước sản xuất (Manufactured Year and Country)\": \"2019, Việt Nam\",\\n        \"Niên hạn SD (Lifetime limit to)\": \"1660\",\\n        \"Kinh doanh vận tải (Commercial Use)\": \"Không\",\\n        \"Cải tạo (Modification)\": \"Không\"\\n    },\\n    \"THÔNG SỐ KỸ THUẬT (SPECIFICATIONS)\": {\\n        \"Công thức bánh xe (Wheel Formula)\": \"4x2\",\\n        \"Vết bánh xe (Wheel Tread)\": \"1599/1600 (mm)\",\\n        \"Kích thước bao (Overall Dimension)\": \"4973 x 1900 x 1500 (mm)\",\\n        \"Kích thước lòng thùng xe (Inside cargo container dimension)\": \"4959 x 1973 x 1150 (mm)\",\\n        \"Chiều dài cơ sở (Wheelbase)\": \"2968 (mm)\",\\n        \"Khối lượng bản thân (Kerb mass)\": \"1795 (kg)\",\\n        \"Khối lượng hàng CC theo TK/CP TGGT (Design/Authorized payload)\": \"\",\\n        \"Khối lượng toàn bộ theo TK/CP TGGT (Design/Authorized total mass)\": \"2330/2330 (kg)\",\\n        \"Khối lượng kéo theo TK/CP TGGT (Design/Authorized towed mass)\": \"\",\\n        \"Số người cho phép chở (Permissible No. of Pers Carried: seat, stood place, laying place)\": \"5 chỗ ngồi, 0 chỗ đứng, 0 chỗ nằm\",\\n        \"Loại nhiên liệu (Type of Fuel Used)\": \"Xăng\",\\n        \"Thể tích làm việc của động cơ (Engine Displacement)\": \"1997 (cm3)\",\\n        \"Công suất lớn nhất/tốc độ quay (Max. output/rpm)\": \"130(kW)/4500vph\",\\n        \"Số sê-ri (No.)\": \"KD-6260523\",\\n        \"Số lượng lốp, cỡ lốp/trục (Number of tires; tire size/axle)\": \"1: 2; 245/45R18\\\\n2: 2; 245/45R18\",\\n        \"Có lắp thiết bị giám sát hành trình (Equipped with Tachograph)\": \"Không\",\\n        \"Không cấp tem kiểm định (Inspection stamp was not issued)\": \"Không\",\\n        \"Ghi chú\": \"Biển đăng ký nền trắng\"\\n    },\\n    \"THÔNG TIN KHÁC\": {\\n        \"Số phiếu kiểm định (Inspection Report No)\": \"2901S-07548/21\",\\n        \"Có hiệu lực đến ngày (Valid until)\": \"14/10/2022\",\\n        \"Issued on Day/Month/Year\": \"Hà Nội, ngày 15 tháng 4 năm 2021\"\\n    }\\n}'}]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a30751c-a3bc-4e0e-ab98-6e031c680b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': [{'type': 'image',\n",
       "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=800x600>},\n",
       "   {'type': 'image',\n",
       "    'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2048x1221>},\n",
       "   {'type': 'text',\n",
       "    'text': 'You are a highly precise assistant specializing in Vietnamese OCR. Your task is to **accurately extract text** from two images of a Citizen Identity Card: the front side and the back side. Please follow these strict requirements:  \\n\\n- Extract text **exactly as it appears** in the image, including capitalization, special characters, and formatting.  \\n- **Do not modify, infer, or guess** any missing values. If a field is unreadable or missing, return an empty string (\"\").  \\n- Ensure all extracted text is **error-free** and corresponds correctly to the predefined fields.\\n- Maintain the exact JSON structure and **output only valid JSON**.\\n- **Note:** The image may be rotated, distorted, damaged, low-resolution, or affected by lighting inconsistencies (e.g., glare, shadows, overexposure).\\n\\n### **JSON Output Format:**\\n{\\n    \"Loại giấy tờ\": \"CCCD\", \\n    \"Số\": \"<str>\", \\n    \"Họ và tên\": \"<str>\", \\n    \"Ngày, tháng năm sinh\": \"<str DD/MM/YYYY>\", \\n    \"Giới tính\": \"<str>\", \\n    \"Quốc tịch\": \"<str>\", \\n    \"Quê quán\": \"<str>\", \\n    \"Nơi thường trú\": \"<str>\", \\n    \"Có giá trị đến\": \"<str DD/MM/YYYY>\", \\n    \"Đặc điểm nhân dạng\": \"<str>\", \\n    \"Ngày cấp\": \"<str DD/MM/YYYY>\", \\n    \"Nơi cấp\": \"<str>\", \\n    \"Người ký\": \"<str Name of the signer\\'s>\"\\n}\\n'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': '{\\n    \"Loại giấy tờ\": \"CCCD\",\\n    \"Số\": \"037087007100\",\\n    \"Họ và tên\": \"NGUYỄN QUỐC CHÍNH\",\\n    \"Ngày, tháng năm sinh\": \"01/10/1987\",\\n    \"Giới tính\": \"Nam\",\\n    \"Quốc tịch\": \"Việt Nam\",\\n    \"Quê quán\": \"Thạch Bình, Nho Quan, Ninh Bình\",\\n    \"Nơi thường trú\": \"165/18 Lê Lai, Phường 1, Vũng Tàu, Bà Rịa - Vũng Tàu\",\\n    \"Có giá trị đến\": \"01/10/2027\",\\n    \"Đặc điểm nhân dạng\": \"Nốt ruồi c.2,2cm trên sau cánh mũi trái\",\\n    \"Ngày cấp\": \"30/05/2016\",\\n    \"Nơi cấp\": \"CỤC TRƯỞNG CỤC CẢNH SÁT ĐKQL CƯ TRÚ VÀ DLQG VỀ DÂN CƯ\",\\n    \"Người ký\": \"Trần Quốc Sáng\"\\n}'}]}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23750e31-b592-4215-8d18-7eb9ddee82d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdb329382f64d829b46850a9b699872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe9f04c9f2a44269de6826e54ad3359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/56.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d60feea5f1849b3955206ce83ad4435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e00d6184864577924c9b06525b3896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb76c76999474066ab268ea0315f2301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b07e47ab31846bfa2a86276abee70c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead13f8501bf4256a69d434fe2f971c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0bbd05d7a64741b85de3099865eab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c943c7f580b8420e9b8304826b5e2652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb2fb959a124447b47811c9cddd0b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6408ab0524934e93a5df8b86ead7065a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3037540e5e05490fb3ccffa54a321460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2a08094cbf4a03a98851365e3edb95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda:0\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "min_pixels = 256 * 28 * 28\n",
    "max_pixels = 1280 * 28 * 28\n",
    "\n",
    "processor = Qwen2VLProcessor.from_pretrained(model_id, min_pixels=min_pixels, max_pixels=max_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bc21ce8-d451-47c8-90f2-619ea515a07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 16 04:37:14 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A800 80GB PCIe          On  |   00000000:01:00.0 Off |                    0 |\n",
      "| N/A   50C    P0             75W /  300W |    5139MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dbb1505-bd63-4878-ace4-3c917df44c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./Qwen2\",  \n",
    "    num_train_epochs=1,  \n",
    "    per_device_train_batch_size=2,  \n",
    "    per_device_eval_batch_size=2,  \n",
    "    gradient_accumulation_steps=8,  \n",
    "    gradient_checkpointing=True,  \n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  \n",
    "    learning_rate=1e-5, \n",
    "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
    "    lr_scheduler_kwargs={\"min_lr\": 1e-6},\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "    # Logging and evaluation\n",
    "    logging_steps=30,  \n",
    "    eval_steps=30,  \n",
    "    eval_strategy=\"steps\", \n",
    "    save_strategy=\"steps\",  \n",
    "    save_steps=60, \n",
    "    metric_for_best_model=\"eval_loss\",  \n",
    "    greater_is_better=False,  \n",
    "    load_best_model_at_end=True, \n",
    "    # Mixed precision and gradient settings\n",
    "    bf16=True,  \n",
    "    tf32=True,\n",
    "    weight_decay=0.05,\n",
    "    max_grad_norm=1.0, \n",
    "    warmup_ratio=0.03,  \n",
    "    # Hub and reporting\n",
    "    push_to_hub=True, \n",
    "    report_to=\"wandb\",  \n",
    "    # Gradient checkpointing settings\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  \n",
    "    # Dataset configuration\n",
    "    dataset_text_field=\"\", \n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  \n",
    "    max_seq_length=None # No truncation applied\n",
    ")\n",
    "\n",
    "training_args.remove_unused_columns = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9cdf4-1522-422e-93d7-56f271313b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Qwen2-VL-ViDocs-final</strong> at: <a href='https://wandb.ai/tungpthe187331-fpt-university/Qwen2-VL-ViDocs/runs/ytx3d4l9' target=\"_blank\">https://wandb.ai/tungpthe187331-fpt-university/Qwen2-VL-ViDocs/runs/ytx3d4l9</a><br> View project at: <a href='https://wandb.ai/tungpthe187331-fpt-university/Qwen2-VL-ViDocs' target=\"_blank\">https://wandb.ai/tungpthe187331-fpt-university/Qwen2-VL-ViDocs</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code></code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250516_043727-8coxs0pb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tungpthe187331-fpt-university/Qwen2-VL-ViDocs/runs/8coxs0pb' target=\"_blank\">Qwen2-VL-ViDocs-final</a></strong> to <a href='https://wandb.ai/tungpthe187331-fpt-university/Qwen2-VL-ViDocs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tungpthe187331-fpt-university/Qwen2-VL-ViDocs' target=\"_blank\">https://wandb.ai/tungpthe187331-fpt-university/Qwen2-VL-ViDocs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tungpthe187331-fpt-university/Qwen2-VL-ViDocs/runs/8coxs0pb' target=\"_blank\">https://wandb.ai/tungpthe187331-fpt-university/Qwen2-VL-ViDocs/runs/8coxs0pb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tungpthe187331-fpt-university/Qwen2-VL-ViDocs/runs/8coxs0pb?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fdab21442e0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Qwen2-VL-ViDocs\",  \n",
    "    name=\"Qwen2-VL-ViDocs\",  \n",
    "    config=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10a15da4-cc42-4be6-b1f9-a068eaf5c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    # Get the texts and images, and apply the chat template\n",
    "    texts = [\n",
    "        processor.apply_chat_template(example, tokenize=False) for example in examples\n",
    "    ]  # Prepare texts for processing\n",
    "    image_inputs = [process_vision_info(example)[0] for example in examples]  # Process the images to extract inputs\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(\n",
    "        text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
    "    )  # Encode texts and images into tensors\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
    "\n",
    "    # Ignore the image token index in the loss computation (model specific)\n",
    "    if isinstance(processor, Qwen2VLProcessor):  # Check if the processor is Qwen2VLProcessor\n",
    "        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n",
    "    else:\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n",
    "\n",
    "    # Mask image token IDs in the labels\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
    "\n",
    "    batch[\"labels\"] = labels  # Add labels to the batch\n",
    "\n",
    "    return batch  # Return the prepared batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9a7e050-c10e-4287-a8bb-b677b2c61be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    #tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d02a5e34-03af-4786-b8a5-35febc601e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/venv/main/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='319' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [319/319 1:25:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.736100</td>\n",
       "      <td>0.220423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.099114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.068127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.052605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.046595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.044232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.041257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.040238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.039574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.044100</td>\n",
       "      <td>0.039552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/venv/main/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/venv/main/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/venv/main/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/venv/main/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=319, training_loss=0.1256866062696451, metrics={'train_runtime': 5147.8073, 'train_samples_per_second': 0.991, 'train_steps_per_second': 0.062, 'total_flos': 1.1013663369272525e+17, 'train_loss': 0.1256866062696451})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0ba7f-9be5-4b68-8f90-86057eb0cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202ae6d-931e-4ee7-9d35-44d23e3e153a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
